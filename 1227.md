# WebCrawling

## 1. urllib

> url 작업을 위한 여러 모듈을 모은 패키지

- urllib.request - url주소의 문서를 열고 일기 위해 사용하는 모듈

  - urllib.request.urlopen().read() - 읽은 데이터를 python 실행환경의 메모리로 로드
  - 응답 코드
    - 200 OK
    - 404 Not Found
    - 500 Server Error

- urllib.parse - url주소의 문서를 구문 분석

  - urllib.parse.urljoin(베이스, 꼬리)

    - 상대경로를 활용할 수 있다.

      ```python
      from urllib.parse import urljoin
      base = "http://example.com/html/a.html"
      print( urljoin(base, "b.html") )
      print( urljoin(base, "../index.html") )
      
      http://example.com/html/b.html
      http://example.com/index.html
      ```

- urllib.error - urllib.request에 의해 발생되는 예외를 포함하고 있는 모듈

- urllib.parse.urlencode(key = value, ...) - url요청 파라미터를 인코딩

- urlretrieve(url, save_path) - url주소의 문서를 다운로드해서 로컬에 저장



## 2. requests

> HTTP request 전송, 세션 유지, 파라미터 인코딩 등의 기능을 제공하는 모듈

get(), post(), head(), put(), delete()

- requests.get(url, headers = , cookies = )

  - Response 객체를 반환

    - Response객체.request - 요청을 보낸 객체에 접근
    - Response객체.status_code - 요청에 대한 응답 코드
    - Response객체.text - 요청에 대한 응답  body 내용을 text  변환
    - Response객체.content - 요청에 대한 응답 body 내용을 byte 변환
    - Response객체.raise_for_status() - 에러 발생(200 OK 응답이 아닌 경우)
    - Response객체.json() - 요청에 대한 응답이 JSON인 경우 딕셔너리 형태로 변환

    #### 세션 생성하기(로그인이 필요한 경우)

    ```python
    import requests            # 로그인을 위한 모듈 추출하기
    from bs4 import BeautifulSoup
    from urllib.parse import urljoin
    # 아이디와 비밀번호 지정하기
    USER = ""
    PASS = ""
    
    session = requests.session()  # 세션 시작하기 
    #session은 requests 객체처럼 활용
    login_info = {                     # 로그인 정보
        "m_id": USER,                 # 아이디 지정
        "m_passwd": PASS           # 비밀번호 지정
    }
    url_login = "http://www.hanbit.co.kr/member/login_proc.php"
    res = session.post(url_login, data=login_info)  #data 전송하기
    res.raise_for_status()            # 오류가 발생하면 예외가 발생합니다.
    
    url_mypage = "http://www.hanbit.co.kr/myhanbit/myhanbit.html"  # 마이페이지에 접근하기  
    res = session.get(url_mypage)
    #res은 response객체이다
    res.raise_for_status()
    
    soup = BeautifulSoup(res.text, 'html.parser')
    mileage = soup.select_one('.mileage_section1 > dd > span').string
    # or: mileage = soup.select_one('.mileage_section1 span')
    e_coin = soup.select_one('.mileage_section2 > dd > span').string
    print('마일리지: ', mileage)
    print('이코인: ', e_coin)
    ```

    

## 3. BeautifulSoup

> HTML, XML 파싱하는 데 사용되는 라이브러리

BeautifulSoup(소스코드, '파서') 형태로 사용한다.

- 네 가지 파서
  - html.parser
  - lxml
  - xml
  - html5lib

